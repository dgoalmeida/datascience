{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Big Data Real-Time Analytics com Python e Spark</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 6 - </font>\n",
    "\n",
    "# <font color='blue'>Machine learning em python</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do problema de negocio\n",
    "\n",
    "Criar um modelo preditivo que possa prever se uma pessoa pode ou não desenvolver diabetes\n",
    "\n",
    "Dataset: Pima Indians Diabetes dataset\n",
    "\n",
    "http://archives/ics.uci.com/ml/datasets/diabetes\n",
    "\n",
    "Registros medicos de pacientes do Pima Indians e cada registro está marcado se paciente teve ou nao diabetes\n",
    "\n",
    "Variáveis:\n",
    "- numero de vezes gravida\n",
    "- concentração de glicose em duas horas em um teste oral de tolerancia à glicose\n",
    "- pressão sanguinea (mm Hg)\n",
    "- espessura da dobra cutânea do triceps (mm)\n",
    "- 2 horas de insulina \n",
    "- indice de massa corporal\n",
    "- linhagem de diabetes\n",
    "- idade (anos)\n",
    "- label (variavel target) 0/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo e carregando dados\n",
    "\n",
    "Existem diversar considerações ao se carregar dados para processo de machine learning. ex, seus dados possui um header (cabeçalho)? se nao possuir, voce precisa definir o titulo de cada coluna, seus arquivos possuem comentarios?, qual o delimitador de cada linha. alguns dados estão entre aspas, simples ou duplas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando arquivo csv usando Numpy\n",
    "import numpy as np\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "prima_data = open(arquivo, 'rb')\n",
    "prima_dados = np.loadtxt(prima_data, delimiter=',')\n",
    "print(prima_dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando arquivo csv com Pandas\n",
    "import pandas as pd\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = pd.read_csv(arquivo, names=colunas)\n",
    "print(dados.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise exploratória dos dados\n",
    "\n",
    "estatistica descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando as primeiras 20 linhas\n",
    "dados.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o numero de linhas for muito grande, o algoritmo pode levar muito tempo para ser treinado. se o numero de registros for muito pequeno, voce pode não ter muitos registros para o treino do seu modelo\n",
    "\n",
    "Se você tiver muitas colunas em seu arquivo, o algoritmo pode apresentar problema de performance devido a alta dimensionalidade.\n",
    "\n",
    "A melhor solução vai depender de cada caso, mas lembrese, treine seu modelo em um subset do seu conjunto de dados maior e depois aplique o modelo a novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisando as dimenões\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tipo de dados é muito importante. Pode ser necessário converter strings, ou colunas com numeros inteiros podem representar variáveis categóricas ou valores ordinários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipo de dados de cada atributo\n",
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumário estatistico\n",
    "dados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em problemas de classificação pode ser necessário balancear as classes. Classes desbalanceadas (volume maior de um dos tipos da classe) são comuns e precisam ser tratados durante a fase de pre-processamento. Podemos ver a baixo que existe uma clara desproporção entre as classes 0 (não ocorrencia de diabetes) e 1 (ocorrencia de diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.groupby('class').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlação é o relacionamento entre duas variáveis. O método mais comum para calcular a correlação é o método de \"pearson\" que assume uma distribuição normal dos dados.Correlação de -1 mostra uma correlação negativa, enquanto uma correlação de +1 mostra uma correlação positiva. uma correlação igual a 0 mostra que não á uma correlação entre as variáveis\n",
    "\n",
    "Alguns algoritmos como regreção linear e regreção logistica podem apresentar problemas deperformance se ouver atributos altamente correlacionados (colineares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.corr('pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skew (ou simetria) se refere a distribuição de dados que é assumida ser normal ou gaussiana (bell curve). Muitos algoritmos de machine learning consideram que os dados possuem uma distribuição normal. Conhecendo a assimetria dos dados, permite que Você faça uma preparação e entregue oque o algoritmo espera receber. aumentando dessa forma a acuracia do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização com matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o histograma podemos rapidamente avaliar a distribuição dos dados para cada atributo. OS histrogramas agrupam os dados em bins e fornecem uma contagem do numero de observações em cada bin. COm o histograma, você pode rapidamente verificar a assimetria  dos dados e se eles estão em uma distribuição normal ou não. isso também vai ajudar na identificação de outliers.\n",
    "\n",
    "Podemos ver que os atributos age, ped e test possuem uma distribuição exponencial. Podemos ver que as colunas mass e press possuem uma distribuição normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os density plots são uma outra forma de visualizar a distribuição dos dados para cada atributo. O plot é uma especia de histograma abstrato com ua curva suave atravez do topo dos bins. de um histograma. Pode ser mais facil identificar a distribuição dos dados usando density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plots univariado\n",
    "dados.plot(kind='density', subplots=True, layout=(3,3), sharex = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os boxplots também podemos revisar a distribuição dos dados para cada atributo. A linha no centro (vermelho) é o valor da mediana (quantil 50%), a linha a baixo é o quantil 25% e a linha a cima é o quantil 75%. O boxplot ajuda a ter uma idéia da dispersão dos dados e os possiveis outliers\n",
    "\n",
    "Podemos ver a dispersão dos dados é bem diferente entre os atributos. As colunas Age, skin e test possuem uma simetria muito proxima a valores de dados menores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.plot(kind='box', subplots=True, layout=(3,3), sharex = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de correlação com nome das variaveis\n",
    "\n",
    "correlation = dados.corr()\n",
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlation, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(colunas)\n",
    "ax.set_yticklabels(colunas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um scatterplot mostra o relacionamento entre duas variaveis como pontos em duas dimensões, sendo um eixo para cada variavel. Podemos criar um scatterplot para cada par de variaveis do dataset. a exemplos da matriz de correlação, o scatterplot matrix é simetrico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dados)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização com seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=dados, orient='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "sns.distplot(dados.pedi, fit = stats.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os dados para machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitos algoritmos esperam receber os dados em um formato especifico. É seu trabalho preparar os dados em uma estrutura que seja adequada ao algoritmo que você esteja utilizando\n",
    "\n",
    "É muito provavel que você tenha que realizar tarefas de pré-processamento nos dados. Esse é um passo necessário no processamento. O desafio é fato que cada algotirmo tem uma estrutura diferente, ou pode requerer diferentes transformações nos dados. Más é possivel, em alguns casos, obter bons resultados sem um trabalho de pre-processamento. Más é uma boa pratica criar diferentes visoes e transformações nos dados, de modo a poder testar diferentes algoritmos de machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização - Método 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma das primeiras tarefas de pré-processamento, é colocar os dados na mesma escala. Muitos algoritmos de machine learning vão se beneficiar disso e produzir resultados melhores. Significa colocar os dados em uma escala entre 0 e 1. Isso é util para otimização, sendo usado no core de MachineLearning, como gradient descent. Isso é util tambem para algoritmos como regreção, redes neurais e algoritmos que usam medidas de distancia, como o KNN. O scikit-learn possui uma função para essa etapa chamado MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados em uma mesma escala, entre 0 e 1\n",
    "\n",
    "# Import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando nova escala (Normalizando os dados)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarizando os dados transformados\n",
    "print(\"Dados originais \\n\\n\", dados.values)\n",
    "print(\"Dados normalizados \\n\\n\", rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização método 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização se refere a ajustar a escala de cada observação (linha) de modo que ela tenha comprimento igual a 1 (chamado de vetor de comprimento 1 em algebra linear). Esse método de pré processamento é util quando temos datasets esparsos (com muitos zeros), e atributos com escalas muito variada. Util quando utilizamos algoritmos de redes neurais ou que usam medidas de distancia. como o KNN. O scikt-leanr possui uma função chamada Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer().fit(x)\n",
    "normalizedX = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarizando os dados transformados\n",
    "print(\"Dados originais \\n\\n\", dados.values)\n",
    "print(\"Dados normalizados \\n\\n\", normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padronização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronização é a técnica de transformar os dados com distribuição Gaussiana (normal) e diferentes médias e desvios padrão em uma distribuição Gaussiana com média 0 e desvio padrão 1. Isso é util para algoritmos que esperam que os dados estejam com uma distribuição Gaussiana, como regreção linear, regreção logistica e linear discriminant analisys. funciona bem quando os dados já estão na mesma escala. O scikt-leanr possui uma função chamada StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(x)\n",
    "standardX = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarizando os dados transformados\n",
    "print(\"Dados originais \\n\\n\", dados.values)\n",
    "print(\"Dados padronizados \\n\\n\", standardX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarização (Transformação dos dados e mvalores binarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós podemos definir um valor em nossos dados, ao qual chamamos de threshold e então definimos que todos os valores a cima do threshold serão marcados como sendo 1 e todos os valores iguais ou menores serão marcados como 0. Isso é oque chamamos de binarização. Isso é util quando temos probabilidades e queremos transformar os dados em algo com mais significado. O scikt-learn possui a função Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=0.2).fit(x)\n",
    "binarizedX = binarizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarizando os dados transformados\n",
    "print(\"Dados originais \\n\\n\", dados.values)\n",
    "print(\"Dados binarizados \\n\\n\", binarizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os atributos presentes no seu dataset e que você utiliza nos dados de treino, terão grande influencia na precisão e resultado do seu modelo preditivo. Atributos irrelevantes terão impacto negativo na performance. enquanto atributos colineares podem afetar o grau de acuracia do modelo. O scikt-learn possui funções que automatizam o trabalho de extração e seleção de variaveis\n",
    "\n",
    "A etapa de feature selection é onde selecionamos os atributos (variáveis) que serão melhores candidatas a variaveis preditoras. O feature selection nos ajuda a reduzir o overfitting (quando o algoritmo aprende demais), aumenta a acurácia do modelo e reduz o tempo de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testes estatisticos podem ser usados para selecionar os atributos que possuem forte relacionamento com a variável que estamos tentando prever. O scikt-learn fornece a função SelectKBest() que pode ser usada com diversos testes estatisticos, para selecionar os atributos. vamos usar o teste qui-quadrado e selecionar as 4 melhores variaveis que possam ser usadas como variaveis preditoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para seleção de variaveis\n",
    "best_var = SelectKBest(score_func=chi2, k=4)\n",
    "\n",
    "# executa a função de pontuação (em X e Y) e obtem os recursos selecionados\n",
    "fit = best_var.fit(x,y)\n",
    "\n",
    "# Reduz x para os recursos selecionados\n",
    "features = fit.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultado\n",
    "print(\"\\nNumero original de features: \", x.shape[1])\n",
    "print(\"\\nNumero reduzido de features: \", features.shape[1])\n",
    "print(\"\\nVariáveis selecionadas\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminação recursiva de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é outra tecnica para seleção de atributos, que recursivamente remove os atributos e constrói o modelo com os atributos remanecentes. Esta técinca utiliza acurácia do modelo para identificar os atributos que mais contribuem para prever a variável alvo. Em ingles essa técnica significa Recursive Feature Elimination FRE\n",
    "\n",
    "O exemplo a baixo utiliza a técnica de eliminação recursiva de atributos com um algoritmo de regreção logistica para selecionar as 3 melhores variáveis preditoras. O FRE selecionou as variaveis preg, mass e pedi que estão marcadas como true em \"Atributos selecionados\" e com valor 1 em \"ranking dos atributos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = LogisticRegression()\n",
    "\n",
    "rfe = RFE(modelo, 3)\n",
    "fit = rfe.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dos resultados\n",
    "print(\"\\nNumero de atributos: \", fit.n_features_)\n",
    "print(\"\\nVariáveis preditoras: \", dados.columns[0:8])\n",
    "print(\"\\nVariaveis selecionadas: \", fit.support_)\n",
    "print(\"\\nRanking dos atributos: \", fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método Ensamble para seleção de variávis\n",
    "\n",
    "Bagged Decision Trees, como o algoritomo Random Forest (esses são chamados de métodos Emsamble) podem ser usados para estimar a importãncia de cada atributo. Esse método retorna um score para cada atributo.\n",
    "\n",
    "Quanto maior o score maior a importancia do atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariáveis preditoras: \", dados.columns[0:8])\n",
    "print(\"\\nScore de variaveis:\",modelo.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de dimencionalidade (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analisys (PCA)\n",
    "\n",
    "o PCA foi inventado em 1901 por Karl Pearson e utiliza agebra linear para transformar datasets em uma forma comprimida, o que é conhecido como redução de dimencionalidade. Com PCA você pode escolher o numero de dimensões (na forma de componente) a partir de um grande conjunto de variáveis disponíveis em um conjunto de dados. Esta técnica permite extrair um numero pequeno de conjuntos dimensionais a partir de um dataset altamente dimensional. Com menos variaveis a visualização também se torna muito mais significativa. PCA é mais util quando se lida com 3 ou mais dimensões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada componente resultante é uma combinação linear de n atributos. Ou seja, cada componente principal é a combinação de componentes dentro do dataset. O primeiro componente principal é a combinação linear dos atributos com máxima variãncia e determina a direção a direção em que hà mais alta variabilidade nos dados. Quanto maior a variabilidade capturada no primeiro componente principal, mais informação será capturada pelo componente, o segundo componente principal captura a variabilidade remanecente. Todos os componentes remanescentes seguem o mesmo conceito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PCA precisa ser alimentado com dados normalizados. Utilizar o PCA em dados não normalizados pode gerar resultados inesperados.\n",
    "\n",
    "A Análise de componentes principais é uma tecnica da estatistica multivariada que consiste em transformar um conjunto de variáveis originais em outro conjunto de variáveis denominadas de componentes principais. Os componentes principais apresentam propriedades importantes. cada componente principal é uma combinação linear de todas as variáveis originais. São independentes entre si e estimados com o proposito de reter, em ordem de estimação, o máximo de informação, e mtermos da variação contida dos dados. Os componentes principais são garantidamente independentes apenas se os dados forem normalmente distribuidos\n",
    "\n",
    "Procura-se redistribuir a variaão observada nos eixos originais, de forma a se obter um conjunto de eixos ortogonais não correlacionados. Esta técinca pode ser utilizada par geração de indices e agrupamento de individuos. A análise agrupa os individuos de acordo com sua variação, isto é os individuos são agrupados segundo sua variãncia, ou seja, segundo seu comportamento dentro da população, representado pelo conjunto de caracteristicas que define o individuo, ou seja a atécnica agrupa os individuos, segundo a variação de suas caracteristicas.\n",
    "\n",
    "A análise de componentes principais é associada a idéia de redução de massa de dados, com menor perda possivel de informação\n",
    "\n",
    "Objetivo é sumarizar os dados que contem muitas variáveis (p) por um conjunto menor de variáveis (k). compostas derivadas a partir do conjunto original. PCA usa um conjunto de dados representados por uma matriz de n registros por p atributos que podem estar correlacionados e sumariza esse conjunto por eixos não correlacionados. que são uma combinação linear das p variáveis originais. as primeiras k variáveis contém a maior quantidade de variação dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em termos gerais o PCA busca reduzir o numero de dimensões de um dataset, projetando os dados em um novo plano. Usando essa nova projeção os dados originais, que podem envolver diversas variáveis, podem ser interpretados usando menos dimenões\n",
    "\n",
    "No dataset reduzido podemos observar com mais clareza tendencias, padrões e/ou outliers. Mas vale lembrar que a regra \"se não está nos dados brutos, não existe\" é sempre valida. A PCA fornece apenas mais claresa as infirmações que já estão lá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledx = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "fit = pca.fit(rescaledx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarixando os componentes\n",
    "print(\"Variância %s\" % fit.explained_variance_)\n",
    "print(\"\\nComponents\",fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostragem - Resampling\n",
    "\n",
    "Você precisa saber se seu modelo preditivo vai funcionar bem quando receber novos dados. A melhor mandeira de validar a performance do modelo é fazer previsões em dados que você já conhece o resultado. Outra maneira de testar a performance do seu modelo é utilizar técnicas de estatistica como método de amostragem que permite você estimar quão bem seu modelo irá fazer previsões em novos dados.\n",
    "\n",
    "A avaliação do modelo é uma estatistica de quão bem o algoritmo será capaz de prever em novos dados. Isso não garante performance. Após avaliar o modelo, nós podemos treinar o modelo novamente com os dados de treino e então preparalo para o uso operacional em produção. Existem diversas técnicas para isso e estudaremos duas, conjunto de dados de treino e teste e, cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados de treino e de teste\n",
    "\n",
    "Este é o método mais utilizado para avaliar a performance de um algoritmo de machine learning. dividimos os dados originais em dados de treino e de teste. treinamos o algoritmo nos dados de treino e, fazemos as previsões nos dados de teeste e avaliamos o resultado. A divisão dos dados vai depender do seu dataset, más utiliza se com frequencia tamanhos entre 70/30 e 65/35 (treino e teste)\n",
    "\n",
    "Este método é bem veloz e ideal para conjunto de dados muito grandes. O ponto negativo é a alta variancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliação usando dados de treino e teste\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# definindo o tamanho das amostras \n",
    "teste_size = 0.33\n",
    "\n",
    "# garante que os resultados podem ser reproduzidos\n",
    "# isso é importante para comparar a acuracia com outros algoritmos de machine learning\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando conjunto de dados de treino e teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(x,y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# criação do modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# treinamento do modelo\n",
    "modelo.fit(X_treino, Y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = modelo.score(X_teste, Y_teste)\n",
    "print(\"Acurácia nos dados de teste: %.3f%%\" % (result * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Cross validation é uma tecnica que pode ser utilizada para avaliar a performance de um modelo com menos variancia do que a tecnica de dividir os dados em treino/teste. com essa tecnica dividims em partes normalmente chamadas de k-folds. Cada parte é chamada fold. Podemos sumarizar a performance em cada fold usando a média e o desvio padrão. O resultado normalmente é mais confiavel, e oferece maior acuracia ao modelo. A chave desse processo está em definir o correto valor de k, de modo que o numero de folds represente adequadamente o numero de repetições necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# definindo o tamanho das amostras \n",
    "seed = 7\n",
    "num_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "resultado = cross_val_score(model, x, y, cv=num_folds)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando performance do modelo\n",
    "\n",
    "As métricas que você escolhe para avaliar a performance do seu modelo vão influenciar a forma como a performance é medida e comparada com modelos criados com outros algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas para algoritmos de classificação\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia\n",
    "\n",
    "# numero de previsões corretas. É util apenas quando existe o mesmo numero de observações em cada classe.\n",
    "resultado = cross_val_score(model, x, y, cv=num_folds, scoring='accuracy')\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curva ROC\n",
    "\n",
    "A curva ROCK permite analisar as metricas AUC (UNDER THE CURVE)\n",
    "Essa é uma métrica de performance para classificação binária, em que podemos definir as classes em positivas e negativas\n",
    "Problemas de classificação binária são um trade-off entre sensitivity e Specifity\n",
    "Sensitivity é a taxa de verdadeiro positivo TP. esse é o numero de instancias positivas da primeira classe que foram previstas\n",
    "Specifity é a taxa de verdadeiro negativo TN. esse é o numero de instancias negativas da segunda classe que foram previstas\n",
    "Valores a cima de 0.5 indicam uma boa taxa de previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = cross_val_score(model, x, y, cv=num_folds, scoring='roc_auc')\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# Permite verificar a acurácia atravez de uma matrix\n",
    "\n",
    "# criando conjunto de dados de treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(x,y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# criação do modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# treinamento do modelo\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "previsoes = modelo.predict(X_teste)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(previsoes, Y_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relatório de clasificação\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de classificação\n",
    "\n",
    "Não temos como saber qual algoritmo vai funcionar melhor na constução do modelo, antes de testarmos o algoritmo no dataset. O ideal é testar alguns algoritmos e então escolher o que oferece melhor nivel de precisão. Vamos testar um conjunto de algoritmos de classificação. nas mesmas condições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regreção logistica\n",
    "\n",
    "Algoritmo Linear. O Algoritmo de regreção logistica assume que seus dados estão normalizados (Em uma distribuição normal) para valores numéricos que podem ser modelados com classificação binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultado = cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant Analisys\n",
    "\n",
    "Algoritmo linear. Técnica estátistica par aclassificação binária. Também assume que os dados estão em uma distribuição normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "num_folds = 5\n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultado = cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN K-nearest Neighbors\n",
    "\n",
    "Algoritmo não linear que utiliza uma métrica de distancia para encontrar o valor de k mais adequado as distãncias do dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "num_folds = 5\n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultado = cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Algoritmo não linear. Calcula a probabilidade de cada classe e a probabilidade condicional de cada classe dado uma variavel de entrada. As probabilidades são então estimadas para o novo dado e multiplicadas. assumindo que são independentes (suposição simples ou naive). Assume dados em distribuição Gaussiana (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "num_folds = 5\n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultado = cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART (Classification and Regretion tree)\n",
    "\n",
    "Algoritmo não linear. O algoritmo CART constrói uma arvore binária do dataset de treino. Cada atributo e cada valor de cada atributo são avaliados com o objetivo de reduzir a função de custo (Cust function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "num_folds = 5\n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultado = cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - suport vector machine \n",
    "\n",
    "Objetivo desse algoritmo é buscar uma linha que melhor separa duas classes dentro de um conjunto de dados. As instãncias de dados que estão mais proximas dessa linha que separa as classes, são chamadas support vectors. O SVM tem sido entendido para suportar multiplas classes\n",
    "\n",
    "Support Vector Machine são algoritmos de classificação  muito poderosos. Quando usados em conjunto com \"Randon Forest\" e outras ferramentas de aprendizagem de máquina, dão uma dimensão muito diferente para a montagem de modelos. Assim eles se tornam cruciais para os casos em que é necessária um poder de previsão muito elevado. Esses algoritmos são um pouco mais dificeis de visualizar devido a complexidade na formulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "num_folds = 5\n",
    "seed = 7\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "resultado = cross_val_score(model, x, y, cv=kfold)\n",
    "\n",
    "print(\"Acurácia Final %.3f%%\" % (resultado.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o resultado do seu modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_treino, X_test, Y_treino, Y_test, = train_test_split(x,y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# criando modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# treinando modelo\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "# salvando modelo\n",
    "arquivo = '9-Arquivos-Cap06/modelos/modelo_classificador_final2.sav'\n",
    "pickle.dump(modelo, open(arquivo, 'wb'))\n",
    "\n",
    "# carregando arquivo\n",
    "modelo_classificador = pickle.load(open(arquivo, 'rb'))\n",
    "modelo_prod = modelo_classificador.score(X_test, Y_test)\n",
    "print('Modelo carregado')\n",
    "\n",
    "# print do resultado\n",
    "print(\"Acurácia %.3f\" % (modelo_prod.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo XGBosst - Extreme Gradient Bossting\n",
    "\n",
    "O algoritmo XGBosst é uama extensão do GBM (Gradient Boosting Method) que permite trabalhar com multithreading em uma unica maquina e processamento paralelo em um cluster de varios servidores. A principal vsntagem do XGBoost sobre o GBM é sua capacidade de gerenciar dados esparsos.\n",
    "O XGBoost automáticamente aceita dados esparsos como input sem armazenar zeros na memória\n",
    "\n",
    "Principais vantagens do XGBosst\n",
    "\n",
    "1 - aceita dados esparsos (oque permite trabalhar com patrizes esparsas), sem a necessidade de converão para matrizes densas. \n",
    "\n",
    "2 - constrói uma arvore de aprendizagem utilizando um moderno método de split (chamado quantile sketch), oque resulta em um tempo de processamento muito menor que métodos tradicionais. \n",
    "\n",
    "3 - Permite computação paralela em uma unica maquina (utilizando multithreading) e processamento paralalo em maquinas distribuidas em cluster.\n",
    "\n",
    "Basicamente o XGboos utiliza os mesmo parâmetros que o GBM e permite tratamento avançado de dados missing\n",
    "\n",
    "O XGBoost é muito utilizado por Cientista de dados que vencem competições do kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar o XGBoost através do Pypi\n",
    "\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# carregando os dados\n",
    "arquivo = '9-Arquivos-Cap06/data/pima-data.csv'\n",
    "colunas = ['preg','plas','pres','skin','test','mass','pedi','age','class']\n",
    "dados = read_csv(arquivo, names=colunas)\n",
    "array = dados.values\n",
    "\n",
    "# separando o array em componentes de imput (x) e output (y)\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_treino, X_test, Y_treino, Y_test, = train_test_split(x,y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "modelo = XGBClassifier()\n",
    "\n",
    "# treinando modelo\n",
    "modelo.fit(X_treino, Y_treino)\n",
    "\n",
    "# print do modelo\n",
    "print(modelo)\n",
    "\n",
    "# fazendo previões\n",
    "y_pred = modelo.predict(X_test)\n",
    "previsoes = [round(value) for value in y_pred]\n",
    "\n",
    "# avaliando as previsões\n",
    "accuracy = accuracy_score(Y_test, previsoes)\n",
    "print(\"Acurácia %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
